{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "## 1.加载和使用预训练模型",
   "id": "f934a055b5478201"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-chinese\")"
   ],
   "id": "6d019d32fcc505d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:33:54.669517Z",
     "start_time": "2025-08-21T08:33:54.664982Z"
    }
   },
   "cell_type": "code",
   "source": "type(model)",
   "id": "a0b07941b1ce4f6d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertModel"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:33:54.738764Z",
     "start_time": "2025-08-21T08:33:54.677041Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModel.from_pretrained(\"./pretrained/bert-base-chinese\")",
   "id": "790977b7a38d79ea",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:33:54.751571Z",
     "start_time": "2025-08-21T08:33:54.747784Z"
    }
   },
   "cell_type": "code",
   "source": "type(model)",
   "id": "b93f1cbe392115d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertModel"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:33:54.763490Z",
     "start_time": "2025-08-21T08:33:54.758577Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "21614ef7794eb39f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:38:54.436903Z",
     "start_time": "2025-08-21T08:38:53.645044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载模型\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-chinese\", num_labels=3)"
   ],
   "id": "2061cd37a4a96512",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:38:56.966252Z",
     "start_time": "2025-08-21T08:38:56.960996Z"
    }
   },
   "cell_type": "code",
   "source": "model",
   "id": "972137ae201d402",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:39:39.779455Z",
     "start_time": "2025-08-21T08:39:39.775416Z"
    }
   },
   "cell_type": "code",
   "source": "type(model)",
   "id": "a8036d24bedc6749",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertForSequenceClassification"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. 加载和使用Tokenizer",
   "id": "cebc8afaaa3387e6"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "markdown",
   "source": "### 2.1 加载",
   "id": "fc1d8af4b44018f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T00:46:11.780838Z",
     "start_time": "2025-08-22T00:45:57.758648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载分词\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")"
   ],
   "id": "3964083fecfa9156",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5a980f64f8e4c1a9b188e4597e78a70"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liubo\\anaconda3\\envs\\nlp\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\liubo\\.cache\\huggingface\\hub\\models--google-bert--bert-base-chinese. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9352c45ccd2f40eca727f4ab72a3c133"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "489e4d1a481842dba47ad63d825e34a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T00:52:17.864755Z",
     "start_time": "2025-08-22T00:52:17.834704Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(\"./pretrained/bert-base-chinese\")",
   "id": "330493a4050e768c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T00:52:27.825658Z",
     "start_time": "2025-08-22T00:52:27.820620Z"
    }
   },
   "cell_type": "code",
   "source": "type(tokenizer)",
   "id": "23383c5e08abffb0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.tokenization_bert_fast.BertTokenizerFast"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 使用",
   "id": "d5a5845e33f480c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:03:24.918218Z",
     "start_time": "2025-08-22T01:03:24.903562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.分词（tokenize）\n",
    "tokens = tokenizer.tokenize(\"我爱自然语言处理\")\n",
    "tokens"
   ],
   "id": "2a3acd320656ec1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '爱', '自', '然', '语', '言', '处', '理']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:04:07.050692Z",
     "start_time": "2025-08-22T01:04:07.046568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2.token->id\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids"
   ],
   "id": "26e6df3a8f64ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2769, 4263, 5632, 4197, 6427, 6241, 1905, 4415]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:04:32.657485Z",
     "start_time": "2025-08-22T01:04:32.652671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3.id->token\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "tokens"
   ],
   "id": "23bb8fc10d300acb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我', '爱', '自', '然', '语', '言', '处', '理']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:17:45.784654Z",
     "start_time": "2025-08-22T01:17:45.778391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4.encode\n",
    "ids = tokenizer.encode(\"我爱自然语言处理\", padding='max_length', max_length=20)\n",
    "ids"
   ],
   "id": "1ad5ba5c529db1a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2769,\n",
       " 4263,\n",
       " 5632,\n",
       " 4197,\n",
       " 6427,\n",
       " 6241,\n",
       " 1905,\n",
       " 4415,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:20:31.815727Z",
     "start_time": "2025-08-22T01:20:31.812205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "text.replace(\" \", \"\")"
   ],
   "id": "56ad0d231c3d2b1a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我爱自然语言处理'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:26:01.256834Z",
     "start_time": "2025-08-22T01:26:01.238153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. __call__\n",
    "inputs = tokenizer(\"我爱自然语言处理\", return_tensors='pt')\n",
    "inputs"
   ],
   "id": "6e5b08cf0679a580",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2769, 4263, 5632, 4197, 6427, 6241, 1905, 4415,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T05:56:43.466388Z",
     "start_time": "2025-08-22T05:56:43.461234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 批量Tokenizer\n",
    "texts = [\"我爱自然语言处理\", \"我爱人工智能\", \"我们一起学习\"]\n",
    "inputs = tokenizer(texts, padding=True)\n",
    "inputs"
   ],
   "id": "dfed8a9b6b2a3748",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2769, 4263, 5632, 4197, 6427, 6241, 1905, 4415, 102], [101, 2769, 4263, 782, 2339, 3255, 5543, 102, 0, 0], [101, 2769, 812, 671, 6629, 2110, 739, 102, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 分词模型配合使用",
   "id": "2678556db8953be1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:53:09.726595Z",
     "start_time": "2025-08-22T01:53:08.337519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "texts = [\"我爱自然语言处理\", \"我爱人工智能\", \"我们一起学习\"]\n",
    "\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "\n",
    "inputs = tokenizer(texts, padding=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs)\n",
    "\n",
    "print(output.last_hidden_state.shape)\n",
    "print(output.pooler_output.shape)"
   ],
   "id": "e567912c6242cfd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 768])\n",
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T01:56:54.360048Z",
     "start_time": "2025-08-22T01:56:52.693521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "texts = [\"我爱自然语言处理\", \"我爱人工智能\", \"我们一起学习\"]\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "\n",
    "inputs = tokenizer(texts, padding=True, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs)\n",
    "\n",
    "print(output.logits.shape)"
   ],
   "id": "ecb2221098e7580b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Datasets",
   "id": "708ce6695f1a5a3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 加载数据集",
   "id": "2f4e609e4dc88a6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T03:21:16.609055Z",
     "start_time": "2025-08-22T03:21:15.243510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_dict = load_dataset(\"csv\", data_files={'train': 'data/raw/train.csv', 'test': 'data/raw/test.csv'})\n",
    "dataset_dict"
   ],
   "id": "c7f6a48e6004aaf4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['cat', 'label', 'review'],\n",
       "        num_rows: 62774\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['cat', 'label', 'review'],\n",
       "        num_rows: 62774\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T02:18:00.429823Z",
     "start_time": "2025-08-22T02:17:59.558421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_dict = load_dataset(\"csv\", data_files='data/raw/train.csv')\n",
    "dataset_dict"
   ],
   "id": "acd32842325d2367",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['cat', 'label', 'review'],\n",
       "        num_rows: 62774\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T02:16:57.900226Z",
     "start_time": "2025-08-22T02:16:57.896965Z"
    }
   },
   "cell_type": "markdown",
   "source": "### 3.2 查看数据集",
   "id": "cb39f6be04ecd8b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T03:21:21.342766Z",
     "start_time": "2025-08-22T03:21:21.338742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 获取Dataset\n",
    "dataset = dataset_dict['train']\n",
    "dataset"
   ],
   "id": "d77c21dcb472661b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['cat', 'label', 'review'],\n",
       "    num_rows: 62774\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T02:19:22.821970Z",
     "start_time": "2025-08-22T02:19:22.814267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 访问行\n",
    "dataset[0]"
   ],
   "id": "30e19932de15f168",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': '书籍',\n",
       " 'label': 1,\n",
       " 'review': '做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一颗年轻的心。我想，这是他能很好的和孩子沟通的一个重要因素。读刘墉的文章，总能让我看到一个快乐的平易近人的父亲，他始终站在和孩子同样的高度，给孩子创造着一个充满爱和自由的生活环境。很喜欢刘墉在字里行间流露出的做父母的那种小狡黠，让人总是忍俊不禁，父母和子女之间有时候也是一种战斗，武力争斗过于低级了，智力较量才更有趣味。所以，做父母的得加把劲了，老思想老观念注定会一败涂地，生命不息，学习不止。家庭教育，真的是乐在其中。'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T03:21:24.069896Z",
     "start_time": "2025-08-22T03:21:24.065114Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[0:3]",
   "id": "b89ec90b2649de53",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': ['书籍', '书籍', '书籍'],\n",
       " 'label': [1, 1, 1],\n",
       " 'review': ['做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一颗年轻的心。我想，这是他能很好的和孩子沟通的一个重要因素。读刘墉的文章，总能让我看到一个快乐的平易近人的父亲，他始终站在和孩子同样的高度，给孩子创造着一个充满爱和自由的生活环境。很喜欢刘墉在字里行间流露出的做父母的那种小狡黠，让人总是忍俊不禁，父母和子女之间有时候也是一种战斗，武力争斗过于低级了，智力较量才更有趣味。所以，做父母的得加把劲了，老思想老观念注定会一败涂地，生命不息，学习不止。家庭教育，真的是乐在其中。',\n",
       "  '作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到真理的火花。整本书的结构颇有特点，从当时（本书写于八十年代）流行的计算机话题引入，再用数学、物理学、宇宙学做必要的铺垫——这些内容占据了大部分篇幅，最后回到关键问题：电脑能不能代替人脑。和现在流行的观点相反，作者认为人的某种“洞察”是不能被算法模拟的。也许作者想说，人的灵魂是无可取代的。',\n",
       "  '作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产率？为什么在文化上有着深刻纽带关系的中国和日本却在经济发展上有着极大的差异？为什么英国的北美殖民地造就了经济强大的美国，而西班牙的北美殖民却造就了范后的墨西哥？……很有价值，但不包括【中国近代史专业】。']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T03:21:24.986633Z",
     "start_time": "2025-08-22T03:21:24.981598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 获取列\n",
    "dataset[0]['review']"
   ],
   "id": "4d4070b9c7981da2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一颗年轻的心。我想，这是他能很好的和孩子沟通的一个重要因素。读刘墉的文章，总能让我看到一个快乐的平易近人的父亲，他始终站在和孩子同样的高度，给孩子创造着一个充满爱和自由的生活环境。很喜欢刘墉在字里行间流露出的做父母的那种小狡黠，让人总是忍俊不禁，父母和子女之间有时候也是一种战斗，武力争斗过于低级了，智力较量才更有趣味。所以，做父母的得加把劲了，老思想老观念注定会一败涂地，生命不息，学习不止。家庭教育，真的是乐在其中。'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.3 预处理数据集",
   "id": "7c8cc7c9b3697920"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T03:21:26.769823Z",
     "start_time": "2025-08-22T03:21:26.765557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.删除列\n",
    "dataset = dataset.remove_columns(['cat'])"
   ],
   "id": "faa849e545fe6eb6",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T03:21:29.342094Z",
     "start_time": "2025-08-22T03:21:29.336151Z"
    }
   },
   "cell_type": "code",
   "source": "dataset.features",
   "id": "b2488dbe240590c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': Value('int64'), 'review': Value('string')}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T03:21:38.528912Z",
     "start_time": "2025-08-22T03:21:38.302399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2.过滤行\n",
    "dataset = dataset.filter(lambda x: x['review'] is not None)"
   ],
   "id": "6a202a024ba47e79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/62774 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ca72958a2fd45468836500251aa5221"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T03:21:42.766793Z",
     "start_time": "2025-08-22T03:21:42.720803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3.划分数据集\n",
    "dataset_dict = dataset.train_test_split(test_size=0.2)\n",
    "dataset_dict"
   ],
   "id": "f6c0bf5eb8b4c773",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 50218\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'review'],\n",
       "        num_rows: 12555\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T03:21:45.431252Z",
     "start_time": "2025-08-22T03:21:45.427884Z"
    }
   },
   "cell_type": "code",
   "source": "dataset_dict['train']",
   "id": "6ee53d90d475cd9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'review'],\n",
       "    num_rows: 50218\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T05:59:51.800044Z",
     "start_time": "2025-08-22T05:59:46.589500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. map操作(逐条)\n",
    "train_dataset = dataset_dict['train']\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "\n",
    "\n",
    "def encode(example):\n",
    "    # example:{label:1,review:'******'}\n",
    "    inputs = tokenizer(example['review'], padding='max_length', max_length=128, truncation=True)\n",
    "    inputs['labels'] = example['label']\n",
    "    return inputs\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(encode, batched=True, remove_columns=['label', 'review'])\n",
    "train_dataset.features"
   ],
   "id": "5357ef74830637f4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/50218 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "879f2d52072b4b26817d526a54b78453"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': List(Value('int32')),\n",
       " 'token_type_ids': List(Value('int8')),\n",
       " 'attention_mask': List(Value('int8')),\n",
       " 'labels': Value('int64')}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T05:58:28.961906Z",
     "start_time": "2025-08-22T05:58:27.332628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. map操作(按批)\n",
    "test_dataset = dataset_dict['test']\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "\n",
    "def batch_encode(batch):\n",
    "    # example:{label:[1,0,1,0],review:['******','******','******','******']}\n",
    "    inputs = tokenizer(batch['review'],padding='max_length', max_length=128, truncation=True)\n",
    "    inputs['labels'] = batch['label']\n",
    "    return inputs\n",
    "\n",
    "test_dataset = test_dataset.map(batch_encode, batched=True, remove_columns=['label', 'review'])"
   ],
   "id": "8118d20f0e8588fd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77244b17199344acab9f130764d356f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 12555\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:07:15.874600Z",
     "start_time": "2025-08-22T06:07:09.987673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. map操作(dataset_dict)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-chinese\")\n",
    "def batch_encode(batch):\n",
    "    # example:{label:[1,0,1,0],review:['******','******','******','******']}\n",
    "    inputs = tokenizer(batch['review'],padding='max_length', max_length=128, truncation=True)\n",
    "    inputs['labels'] = batch['label']\n",
    "    return inputs\n",
    "\n",
    "dataset_dict = dataset_dict.map(batch_encode, batched=True, remove_columns=['label', 'review'])"
   ],
   "id": "3ff7eddab184e06d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/50218 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b339d9fa703a4ae5932f8f8e56f1586f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.4 保存数据集",
   "id": "3e381f1d0c136d1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:08:04.219122Z",
     "start_time": "2025-08-22T06:08:04.129374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.arrow\n",
    "dataset_dict.save_to_disk('data/processed')"
   ],
   "id": "38c296a3bb3c4540",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50218 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7900f4a58b4f4486916342a34dfdb639"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12555 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "288fb11acf1f42ac884825bad07f59e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:11:17.290591Z",
     "start_time": "2025-08-22T06:11:17.275059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('data/processed/train')\n",
    "type(dataset)"
   ],
   "id": "45228d02fa19a0df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:13:07.875374Z",
     "start_time": "2025-08-22T06:13:06.345392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2.json\n",
    "train_dataset.to_json('data/processed/json/train.jsonl')"
   ],
   "id": "e5710a029a509d43",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/51 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6599fd8218614451a740f75fb8cd7ab4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "48627392"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:14:43.308293Z",
     "start_time": "2025-08-22T06:14:19.226350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3.csv\n",
    "train_dataset.to_csv('data/processed/csv/train.csv')"
   ],
   "id": "78e5a0a17c18a4d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/51 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46acc9b135a4481f904b873cf5f3e31f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "59536773"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.5 集成Dataloader",
   "id": "c40f12d72f8fd863"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:21:53.820556Z",
     "start_time": "2025-08-22T06:21:53.806884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = load_from_disk('data/processed/train')\n",
    "train_dataset.set_format(type='torch')\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ],
   "id": "51499b87604aa1a8",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:22:33.511693Z",
     "start_time": "2025-08-22T06:22:33.492603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ],
   "id": "f755caa24b18c456",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7231, 1166,  ...,    0,    0,    0],\n",
      "        [ 101, 6432, 1962,  ...,    0,    0,    0],\n",
      "        [ 101, 1778,  782,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2140, 6564,  ...,    0,    0,    0],\n",
      "        [ 101, 2523, 1962,  ...,    0,    0,    0],\n",
      "        [ 101, 3221, 3633,  ..., 3322,  117,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 1, 1])}\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6c12b0aab8ed4b02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
